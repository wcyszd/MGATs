model_name: "GAT"
num_layers: 2
hidden_size: 8

# Number of attention heads for each layer.
num_heads:
    - 8
    - 1

input_dropout_prob: 0.6
coefficient_dropout_prob: 0.3

alt_opt: false  # whether or not to use alternating optimization between the filter weights and the attention
    # weights. Note that this is not used in the original implementation, but employed here to compare GAT to MoNet
    # since they both use the same multi-headed architecture.

learning_rate: 0.01
weight_decay: 0.01
